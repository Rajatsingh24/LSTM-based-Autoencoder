{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elbxLlGrvVaA",
        "outputId": "b7dc65de-81d9-4a74-fe94-6527d0d01b0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Rajatsingh24/LSTM-based-Autoencoder.git\n",
        "!mv \"/content/LSTM-based-Autoencoder\"/* \"/content\"/\n",
        "!rm -rf \"/content/LSTM-based-Autoencoder\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOLfm6rrIZiq",
        "outputId": "f4c8b2e6-bf77-4dae-8db3-8b4cb0c84cbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'LSTM-based-Autoencoder' already exists and is not an empty directory.\n",
            "mv: cannot stat '/content/LSTM-based-Autoencoder/*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "bDzDE85cvPuB",
        "outputId": "0b95cb74-bfb3-4b2e-e48d-9867c8ee275b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c4785315a846d3e4d4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c4785315a846d3e4d4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "import pickle\n",
        "import zipfile\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from pandas.plotting import andrews_curves\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "###################################################### Preprocessing #####################################################################\n",
        "def preprocess_dataframe(df, target_column=None, fill_method='mean', drop_na=True, sequence_length=32, test_size=0.2, batch_size = 128):\n",
        "    \"\"\"\n",
        "    Loads a DataFrame from a file, preprocesses it, prepares it for LSTM data.\n",
        "    If a target_column is provided, that column is used as the target (y).\n",
        "    Otherwise, it prepares data for an autoencoder (no separate y).\n",
        "    1. Loads file and checks for the target columns\n",
        "    2. Drops any NaN rows and non numeric columns.\n",
        "    3. Fills the NaN values with given method.\n",
        "    4. After preprocessing, data is transformed to fit in lstm.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the data file (e.g., CSV, Excel).\n",
        "        target_column (str, optional): Name of the target column. If provided, use this as target. Otherwise, treats as autoencoder.  Defaults to None.\n",
        "        fill_method (str, optional): Method for filling NaNs: 'mean', 'median', 'most_frequent', or 'constant'.\n",
        "                                       Defaults to 'mean'. If 'constant', `fill_value` must be set.\n",
        "        drop_na (bool, optional): Whether to drop rows with any NaN values. Defaults to True.\n",
        "        sequence_length (int): The length of the sequence to create (e.g., number of features to treat as a sequence).\n",
        "        test_size (float): The proportion of data to use for testing.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_loader, test_loader, input_size) if no target_column.\n",
        "               (train_loader, test_loader, input_size, target_column_name) if target_column provided\n",
        "        A tuple containing:\n",
        "            - train_loader (DataLoader): DataLoader for training data.\n",
        "            - test_loader (DataLoader): DataLoader for test data.\n",
        "            - input_size (int): Number of features.\n",
        "            - target_column_name (str): The name of the target column only when there is target column.\n",
        "    \"\"\"\n",
        "    # 1. Target Column Check\n",
        "    target_col = None\n",
        "    if target_column:\n",
        "        if target_column in df.columns:\n",
        "            target_col = target_column\n",
        "            print(f\"Target column '{target_column}' found.\")\n",
        "        else:\n",
        "            target_column = None  # Reset target_column so we treat as autoencoder\n",
        "    else:\n",
        "        print(\"No target column specified. Treating as autoencoder.\")\n",
        "\n",
        "    #2. Drop Rows with NaNs before Fill\n",
        "    if drop_na:\n",
        "        print(\"Dropping rows with any NaN values...\")\n",
        "        df = df.dropna()\n",
        "\n",
        "\n",
        "    # 3. Drop Non-Numeric Columns (Except Target)\n",
        "    columns_to_drop = []\n",
        "    for col in df.columns:\n",
        "        if col != target_col and not pd.api.types.is_numeric_dtype(df[col]):\n",
        "             columns_to_drop.append(col) #exclude the target column if target column is not numeric\n",
        "    if columns_to_drop:\n",
        "        print(f\"Dropping non-numeric columns: {columns_to_drop}\")\n",
        "        df = df.drop(columns=columns_to_drop)\n",
        "    else:\n",
        "        print(\"No non-numeric columns found.\")\n",
        "\n",
        "\n",
        "    # 4. Handle Missing Values (Only in Numeric Columns After Dropping)\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns #select numeric columns after non-numeric columsn removed\n",
        "    if df[numeric_cols].isnull().any().any():  # Check if any NaN values exist (in numeric columns)\n",
        "        print(\"Handling missing values...\")\n",
        "        if fill_method in ['mean', 'median', 'most_frequent', 'constant']:\n",
        "            imputer = SimpleImputer(strategy=fill_method)\n",
        "\n",
        "            if fill_method == 'constant':\n",
        "                 imputer = SimpleImputer(strategy=fill_method, fill_value=0) #only with constant filling value must be provided\n",
        "\n",
        "            df[numeric_cols] = imputer.fit_transform(df[numeric_cols])  # Apply only to numeric columns\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid fill_method. Choose 'mean', 'median', 'most_frequent', or 'constant'.\")\n",
        "\n",
        "    # Droping NaN and inf\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    if target_col:\n",
        "        inputdf = df.drop(columns=[target_col])\n",
        "        outputdf = df[target_col].apply(lambda x: 0 if x.lower() == 'benign' else 1)\n",
        "        malinputdf = inputdf[outputdf == 1]\n",
        "        beninputdf = inputdf[outputdf == 0]\n",
        "        sample_size = min(len(beninputdf), len(malinputdf), 500)\n",
        "        bensample = beninputdf.sample(n=sample_size, random_state=42)\n",
        "        bensample['Label'] = 'Benign'\n",
        "        malsample = malinputdf.sample(n=sample_size, random_state=42)\n",
        "        malsample['Label'] = 'Malicious'\n",
        "        sample = pd.concat([bensample, malsample])\n",
        "        data = beninputdf.values\n",
        "    else:\n",
        "        inputdf = df\n",
        "        sample_size = min(len(inputdf), 500)\n",
        "        sample = df.sample(n=sample_size, random_state=42)\n",
        "        data = inputdf.values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    if target_col:\n",
        "      X_train = data\n",
        "      data = malinputdf.values\n",
        "      data = scaler.transform(data)\n",
        "      X_test = data\n",
        "    else:\n",
        "      X_train = data\n",
        "\n",
        "    class TabularDatasetTest(Dataset):\n",
        "      def __init__(self, data):\n",
        "          self.data = data.clone().detach()\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.data)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          return self.data[idx], self.data[idx]\n",
        "\n",
        "    class TabularDatasetTrain(Dataset):\n",
        "      def __init__(self, data, sequence_length):\n",
        "          self.data = data.clone().detach()\n",
        "          self.sequence_length = sequence_length\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.data) - self.sequence_length + 1\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          return self.data[idx:idx + self.sequence_length], self.data[idx:idx + self.sequence_length]\n",
        "\n",
        "    if target_column:\n",
        "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "        train_dataset = TabularDatasetTrain(X_train, sequence_length = sequence_length)\n",
        "        test_dataset = TabularDatasetTest(X_test)\n",
        "        train_DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_Dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        return {\n",
        "            'train_loader': train_DataLoader,\n",
        "            'test_loader': test_Dataloader,\n",
        "            'input_df': inputdf,\n",
        "            'target_df': outputdf,\n",
        "            'malinput_df': malinputdf,\n",
        "            'beninput_df': beninputdf,\n",
        "            'target_col': target_col,\n",
        "            'scaler': scaler,\n",
        "            'sample': sample\n",
        "        }\n",
        "    else:\n",
        "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "        train_dataset = TabularDatasetTrain(X_train, sequence_length = sequence_length)\n",
        "        train_DataLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        sample[\"Label\"] = \"dummy_class\"\n",
        "        return {\n",
        "            'train_loader': train_DataLoader,\n",
        "            'test_loader': None,\n",
        "            'input_df': inputdf,\n",
        "            'malinput_df': None,\n",
        "            'beninput_df': None,\n",
        "            'target_df': None,\n",
        "            'target_col': None,\n",
        "            'scaler': scaler,\n",
        "            'sample': sample\n",
        "        }\n",
        "################################################## Model #############################################################################\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, isCuda):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bottleneck_size = int(input_size/2)\n",
        "\n",
        "        self.isCuda = isCuda\n",
        "        self.lstm1 = nn.LSTM(input_size, int(hidden_size/2), num_layers, batch_first=True, bidirectional = True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.lstm2 = nn.LSTM(hidden_size, self.bottleneck_size, num_layers, batch_first=True)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        intermediate_state, hidden = self.lstm1(inputs)#, (h0_1, c0_1))\n",
        "        intermediate_state = self.relu(self.dropout(intermediate_state))\n",
        "        encoded_input, hidden = self.lstm2(intermediate_state)#, (h0_2, c0_2))\n",
        "        return encoded_input, intermediate_state\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers, isCuda):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bottleneck_size = int(output_size/2)\n",
        "\n",
        "        self.isCuda = isCuda\n",
        "        self.lstm2 = nn.LSTM(self.bottleneck_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.lstm1 = nn.LSTM(2*hidden_size, output_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, encoded_input, intermediate_state):\n",
        "        encoded_input, hidden = self.lstm2(encoded_input)#, (h0_2, c0_2))\n",
        "        inputs = torch.cat((self.dropout(encoded_input), intermediate_state), dim=2)\n",
        "        inputs = self.relu(inputs)\n",
        "        decoded_output, hidden = self.lstm1(inputs)#, (h0_1, c0_1))\n",
        "        # print(f\"output: {decoded_output}\")\n",
        "        return decoded_output\n",
        "\n",
        "class LSTMAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, isCuda=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        super(LSTMAE, self).__init__()\n",
        "        hidden_size = hidden_size if hidden_size%2==0 else hidden_size+1\n",
        "        self.encoder = EncoderRNN(input_size, hidden_size, num_layers, isCuda)\n",
        "        self.decoder = DecoderRNN(hidden_size, input_size, num_layers, isCuda)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes the weights of the linear, LSTM, and convolutional layers\n",
        "        using appropriate initialization schemes.\n",
        "        \"\"\"\n",
        "        for m in self.modules():  # Iterate through all modules in the network\n",
        "            if isinstance(m, nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        if 'ih' or 'hh' in name:\n",
        "                            nn.init.xavier_uniform_(param.data)  # Input-to-hidden\n",
        "                    elif 'bias' in name:\n",
        "                        nn.init.zeros_(param.data)\n",
        "\n",
        "    def forward(self, input):\n",
        "        encoded_input, intermediate_state = self.encoder(input)\n",
        "        decoded_output = self.decoder(encoded_input, intermediate_state)\n",
        "        return decoded_output\n",
        "\n",
        "\n",
        "\n",
        "############################################## Andrews Curves ###########################################################################\n",
        "def make_better_andrews_curves(df, class_column, colors=None, plot_title=\"Andrews Curves\",\n",
        "                               line_width=0.8, transparency=0.5,  sample_size=None, legend_loc='best',\n",
        "                               custom_labels=None,  x_axis_ticks=None, x_axis_labels=None,\n",
        "                               figsize=(10, 6), dpi=300, name = \"andrews_curves\"):\n",
        "    \"\"\"\n",
        "    Generates an Andrews Curves plot with enhanced styling.\n",
        "\n",
        "    Args:\n",
        "        df: pandas DataFrame containing the data.\n",
        "        class_column: Name of the column containing class labels.\n",
        "        colors: List of colors to use for each class (e.g., ['blue', 'red']).  Defaults to matplotlib's defaults if None.\n",
        "        plot_title: Title of the plot.\n",
        "        line_width: Width of the lines.\n",
        "        transparency: Alpha value (transparency) of the lines.\n",
        "        sample_size: If an integer is provided, a random sample of the data will be used.  Useful for large datasets.\n",
        "        legend_loc: Location of the legend (e.g., 'best', 'upper right', 'lower left').\n",
        "        custom_labels: A dictionary mapping original class labels to more descriptive labels for the legend.\n",
        "        x_axis_ticks: A list of tick positions for the x-axis. If None, default ticks are used.\n",
        "        x_axis_labels: A list of labels for the x-axis ticks. Must be the same length as x_axis_ticks.\n",
        "        figsize: Tuple specifying the figure size (width, height) in inches.\n",
        "    \"\"\"\n",
        "\n",
        "    if sample_size and sample_size < len(df):\n",
        "        df = df.sample(n=sample_size, random_state=42)  # Sample for faster plotting\n",
        "\n",
        "    plt.figure(figsize=figsize)  # Set the figure size before plotting\n",
        "\n",
        "    ax = andrews_curves(df, class_column, color=colors)  # Store the Axes object\n",
        "\n",
        "    plt.title(plot_title, fontsize=16)\n",
        "    plt.xlabel(\"t\", fontsize=12)  # Added x-axis label\n",
        "    plt.ylabel(\"f(t)\", fontsize=12) # Added y-axis label\n",
        "\n",
        "    for line in ax.get_lines():\n",
        "        line.set_linewidth(line_width)\n",
        "        line.set_alpha(transparency)\n",
        "\n",
        "    # Customize Legend\n",
        "    if custom_labels:\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        new_labels = [custom_labels.get(label, label) for label in labels] # Use .get() to handle missing labels\n",
        "        ax.legend(handles, new_labels, loc=legend_loc, fontsize=10)\n",
        "    else:\n",
        "        plt.legend(loc=legend_loc, fontsize=10)\n",
        "\n",
        "\n",
        "    # Customize X-axis ticks and labels\n",
        "    if x_axis_ticks:\n",
        "        plt.xticks(x_axis_ticks, x_axis_labels)\n",
        "\n",
        "    plt.grid(False)  # Add a grid\n",
        "    plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "    plt.savefig(f\"{name}.png\", dpi=dpi)\n",
        "################################################# Model Training ######################################################################\n",
        "def train_model(model, train_loader, test_loader = None, learning_rate=0.001, epochs=10):\n",
        "    criterion = nn.MSELoss()\n",
        "    info = \"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_loss_data = {}\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        epoch_train_losses = []\n",
        "        mse_losses = []\n",
        "        for i,(inputs, targets) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            # l1_lambda = 0.001\n",
        "            # l2_lambda = 0.0001\n",
        "            # l1_norm = sum(p.abs().sum() for p in model.parameters())  # L1 norm\n",
        "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())  # L2 norm\n",
        "            loss = criterion(outputs, targets)# + l2_lambda * l2_norm + l1_lambda * l1_norm\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if epoch == epochs-1:\n",
        "              mse_loss = F.mse_loss(targets, outputs, reduction='none')\n",
        "              mse_loss_per_data_point = mse_loss.mean(dim=-1)\n",
        "              mse_losses.extend(mse_loss_per_data_point.tolist())\n",
        "            epoch_train_losses.append(loss.item())\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        if test_loader and epoch%1==0:\n",
        "          model.eval()\n",
        "          test_loss = 0.0\n",
        "          with torch.no_grad():\n",
        "              for i,(inputs, targets) in enumerate(test_loader):\n",
        "                  inputs = inputs.to(device)\n",
        "                  targets = targets.to(device)\n",
        "                  outputs = model(inputs.unsqueeze(1))\n",
        "                  loss = criterion(outputs.squeeze(1), targets)\n",
        "                  test_loss += loss.item()\n",
        "\n",
        "          test_loss /= len(test_loader)\n",
        "        else:\n",
        "          test_loss = 0.0\n",
        "        info += f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\\n\"\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "        train_loss_data[f'Epoch {epoch + 1}'] = epoch_train_losses\n",
        "    train_loss_df = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in train_loss_data.items()]))\n",
        "    return model, train_loss_df, mse_losses, info\n",
        "\n",
        "#########################################################################################################################################\n",
        "def detect_anomalies(csv_file, sample_choice=\"Custom Data\", data_slicing_percentage=80, epochs=3, threshold_factor=1.0):\n",
        "  images = []\n",
        "  anomaly_summary = \"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if os.path.exists(\"Results\"):\n",
        "    shutil.rmtree(\"Results\")\n",
        "  os.mkdir(\"Results\")\n",
        "  if sample_choice == \"Custom Data\":\n",
        "    anomaly_summary += f\"[INFO] Loading Custom Dataset {data_slicing_percentage}%...\\n\"\n",
        "    dataframe = pd.read_csv(csv_file.name).sample(frac=data_slicing_percentage/100, random_state=42).reset_index(drop=True)\n",
        "    anomaly_summary += f\"[INFO] Preprocessing Dataset...\\n\"\n",
        "    if dataframe.get('Label') is not None:\n",
        "        processed_data = preprocess_dataframe(dataframe, target_column=\"Label\")\n",
        "    else:\n",
        "        processed_data = preprocess_dataframe(dataframe)\n",
        "        anomaly_summary += f\"[WARNING] No Label Column Found, Using Unsupervised Learning...\\n\"\n",
        "    anomaly_summary += f\"[INFO] Generating Andrews Curves...\\n\"\n",
        "    make_better_andrews_curves(processed_data['sample'], 'Label',\n",
        "                              colors=['Blue', 'Red'],\n",
        "                              plot_title=\"Dataset Andrews Curves\",\n",
        "                              line_width=1.2,\n",
        "                              transparency=0.7,\n",
        "                              legend_loc='upper right',\n",
        "                              figsize=(12, 7),\n",
        "                              name = \"Results/Dataset_andrews_curves\")\n",
        "    images.append(\"Results/Dataset_andrews_curves.png\")\n",
        "    model = LSTMAE(len(processed_data[\"input_df\"].columns),128).to(device)\n",
        "    model.to(device)\n",
        "    anomaly_summary += f\"[INFO] Training Model...\\n\"\n",
        "    _, train_loss_df, mse_losses, info = train_model(model, processed_data['train_loader'], processed_data['test_loader'],epochs=epochs)\n",
        "    anomaly_summary += info\n",
        "    anomaly_summary += f\"[INFO] Saving model, scaler, Dataset Used...\\n\"\n",
        "    dataframe.to_csv('Results/Original_dataset.csv', columns=dataframe.columns, index=False)\n",
        "    pickle.dump(processed_data['scaler'], open('Results/scaler.pkl', 'wb'))\n",
        "    torch.save(model, 'Results/model.pth')\n",
        "    anomaly_summary += f\"[INFO] Generating Loss Curves...\\n\"\n",
        "    plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
        "    for column in train_loss_df.columns:\n",
        "        plt.plot(train_loss_df[column], label=column)\n",
        "    plt.xlabel(\"Batch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss per Epoch\")\n",
        "    plt.legend()  # Show the legend to identify each epoch\n",
        "    plt.grid(True)  # Add a grid for easier reading\n",
        "    plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "    plt.savefig(\"Results/loss_curves.png\", dpi=300)\n",
        "    images.append(\"Results/loss_curves.png\")\n",
        "    Q1, Q3 = np.percentile(mse_losses, [25, 75])\n",
        "    Dict = {\"Q1\": Q1, \"Q3\": Q3}\n",
        "    pickle.dump(Dict, open('Results/INFO.pkl', 'wb'))\n",
        "\n",
        "  else:\n",
        "    Q1, Q3 = 0.19226229563355446, 0.7454282641410828\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - threshold_factor * IQR\n",
        "    upper_bound = Q3 + threshold_factor * IQR\n",
        "    # print(lower_bound, upper_bound)\n",
        "    data_path = os.path.join(os.path.abspath('Data'),sample_choice)\n",
        "    dataframe = pd.read_csv(data_path).sample(frac=data_slicing_percentage/100, random_state=42).reset_index(drop=True)\n",
        "    anomaly_summary += f\"[INFO] Saving model, scaler, Dataset Used...\\n\"\n",
        "    dataframe.to_csv('Results/Scaled_dataset.csv', columns=dataframe.columns, index=False)\n",
        "    scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
        "    original_df = scaler.inverse_transform(dataframe.iloc[:,:-1])\n",
        "    original_df = pd.DataFrame(original_df, columns=dataframe.columns[:-1])\n",
        "    original_df['Label'] = dataframe['Label']\n",
        "    original_df.to_csv('Results/Original_dataset.csv', columns=dataframe.columns, index=False)\n",
        "    shutil.copy('scaler.pkl', 'Results/scaler.pkl')\n",
        "    shutil.copy('model.pth', 'Results/model.pth')\n",
        "    # andrew curve of dataset\n",
        "    anomaly_summary += f\"[INFO] Generating Andrews Curves...\\n\"\n",
        "    make_better_andrews_curves(dataframe, 'Label',\n",
        "                               colors=['Blue', 'Red'],\n",
        "                               plot_title=\"Dataset Andrews Curves\",\n",
        "                               line_width=1.2,\n",
        "                               transparency=0.7,\n",
        "                               legend_loc='upper right',\n",
        "                               figsize=(12, 7),\n",
        "                               name = \"Results/Dataset_andrews_curves\")\n",
        "    images.append(\"Results/Dataset_andrews_curves.png\")\n",
        "    inputdf = torch.tensor(dataframe.iloc[:,:-1].to_numpy(), dtype=torch.float32, device=device)\n",
        "    outputdf = dataframe['Label']\n",
        "    model = torch.load(\"model.pth\",weights_only = False, map_location=device)\n",
        "    model.eval()\n",
        "    outputs = model(inputdf.unsqueeze(1)).squeeze(1)\n",
        "    mse_loss = F.mse_loss(outputs, inputdf, reduction='none')\n",
        "    mse_loss_per_data_point = mse_loss.mean(dim=-1)\n",
        "    anomaly_scores = pd.DataFrame({'Loss': mse_loss_per_data_point.detach().cpu().numpy(), 'Label': outputdf})\n",
        "    anomaly_scores['Anomaly'] = anomaly_scores['Loss'].apply(lambda x: 1 if x > upper_bound else 0)\n",
        "    anomaly_scores['Label'] = anomaly_scores['Label'].apply(lambda x: 1 if x == \"Malicious\" else 0)\n",
        "    out_confusion_matrix = confusion_matrix(anomaly_scores['Label'], anomaly_scores['Anomaly'])\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=out_confusion_matrix, display_labels=[\"Benign\",\"Malignant\"])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(f\"Results/confusion_matrix.png\", dpi=300)\n",
        "    images.append(\"Results/confusion_matrix.png\")\n",
        "    accuracy = accuracy_score(anomaly_scores['Label'], anomaly_scores['Anomaly'])\n",
        "    precision = precision_score(anomaly_scores['Label'], anomaly_scores['Anomaly'])\n",
        "    recall = recall_score(anomaly_scores['Label'], anomaly_scores['Anomaly'])\n",
        "    f1 = f1_score(anomaly_scores['Label'], anomaly_scores['Anomaly'])\n",
        "    # print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "    anomaly_summary += f\"[RESULT] Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\"\n",
        "    anomaly_summary = anomaly_summary + f\"Confusion Matrix:\\n{out_confusion_matrix}\\n\"\n",
        "\n",
        "  folder_path = \"Results\"\n",
        "  with zipfile.ZipFile(\"Results.zip\", 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            relative_path = os.path.relpath(file_path, folder_path)\n",
        "            zipf.write(file_path, relative_path)\n",
        "\n",
        "  return anomaly_summary, images, \"Results.zip\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=detect_anomalies,\n",
        "    inputs=[\n",
        "        gr.File(file_types=[\".csv\"], label=\"Upload CSV File\"),\n",
        "        gr.Radio([\"Benign500.csv\", \"Malignant500.csv\", \"Balance1000.csv\", \"Custom Data\"], value=\"Custom Data\", label=\"Choose Samples or CustomData\"),\n",
        "        gr.Slider(minimum=10, maximum=100, step=10, value=80, label=\"Data Usage Percentage (Training or Detection)\"),\n",
        "        gr.Slider(minimum=1, maximum=20, step=1, value=3, label=\"Training Epochs (Default value is 3)\"),\n",
        "        gr.Slider(minimum=0, maximum=5, step=0.5, value=1.5, label=\"Loss Threshold (x, higher x means high threshold) = Q3 + x*IQR\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Anomaly Summary\"),\n",
        "        gr.Gallery(label=\"Anomaly Plots\"),\n",
        "        \"file\",\n",
        "    ],\n",
        "    title=\"Your own Anomaly Detector\",\n",
        "    description=\"\"\"\n",
        "    ### Fully Unsupervised Anomaly Detection Tool (uses Bidirectional based Autoencoder with skip conn. and Dropout Layers)\n",
        "    ##### Download *\"Result.zip\"* (contains model.pkl, dataset images, output images) to download the results from Right Bottom.\n",
        "    Upload a *CSV file* (Custom Anomalies Detection: Use Output Column: \"Label\" or ), or Use *our trained model*.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "G2SVjekxv9F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4SUpcpo4kGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}